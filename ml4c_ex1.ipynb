{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml4c_ex1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/retsukothedevcat/hello-world/blob/master/ml4c_ex1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "SKJC5ba-oTLC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$\n",
        "\\DeclareMathOperator{\\Prob}{Pr}\n",
        "\\renewcommand{\\vec}[1]{{\\underline{#1}}}\n",
        "\\newcommand{\\dott}[2]{{{#1}^T{#2}}}\n",
        "\\newcommand{\\mat}[1]{{\\mathbf{#1}}}\n",
        "$\n",
        "\n",
        "# Programming Exercises - Set 1\n",
        "This exercise set regards linear regression. You are given one one-dimensional data set **dataset1_linreg.py**. Please import the file to the notebook (click on the arrow on the right side of the screen to open a menu. In the menu, go to Files. Select upload and upload the dataset file to the colab. Now the file can be used in the notebook) We are going to use the *gradient descent* (GD) algorithm during all exercises. For each of the exercises you may have to adjust the number of iterations and the step size of the GD to get a good performance. One technique to verify if the GD is behaving properly is to plot the cost versus the number of iterations. For plotting we will use *matplotlib* library (and Python3). During implementation and debugging it is helpful to carefully observe the dimensions of the objects which enter/leave the calculations.\n",
        "\n",
        "For code brevity, we are going to use slightly different notation than during the lecture. Instead of the separate bias term $b$ in the linear regression, i.e.,\n",
        "\\begin{align}\n",
        "z & = \\sum_{k=1}^{M} w_k x_k + b \\\\\n",
        "& =  \\dott{\\vec{w}}{\\vec{x}} + b,\n",
        "\\end{align}\n",
        "we are going to replace $b$ with $w_0$ and store it inside the parameters vector $\\vec{w}$. It may be conveninet to exted the data vector $\\vec{x}$ by additional one for some calculations. That is, you may use the following notation\n",
        "\\begin{align}\n",
        "z & = \\sum_{k=1}^{M} w_k x_k + w_0 \\\\\n",
        "&=\n",
        "\\begin{bmatrix}\n",
        "w_0 w_1 \\dots w_M \n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "x_1 \\\\\n",
        "\\vdots \\\\\n",
        "x_M \\\\\n",
        "\\end{bmatrix} \\\\\n",
        "& =  \\dott{\\vec{w}}{\\vec{x}}.\n",
        "\\end{align}\n",
        "The inputs in the dataset are organized as a matrix:\n",
        "\\begin{align}\n",
        "\\begin{bmatrix}\n",
        "&\\vec{x}_1 &\\dots &\\vec{x}_N\n",
        "\\end{bmatrix},\n",
        "\\end{align}\n",
        "and the output in the dataset are organized as a matrix: \n",
        "\\begin{align}\n",
        "\\begin{bmatrix}\n",
        "&y_1 &\\dots &y_N\n",
        "\\end{bmatrix}.\n",
        "\\end{align}"
      ]
    },
    {
      "metadata": {
        "id": "wdXi8UdjfUfE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem 1 - Linear Regression\n",
        "We start from importing the data set and ploting the data points. The problem we aim to solve is linear regression. That is, we want to develop a model for predicting the values $y \\in \\mathcal{R}$ from the data $x \\in \\mathcal{R}$. The plot presents also the ground truth, that is, the function from which the training samples were generated (samples also contain extra noise)."
      ]
    },
    {
      "metadata": {
        "id": "XitN8JyJvFrV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "9cefc283-7ab3-4a46-fe56-08337f3ead7f"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset1_linreg import DataSet\n",
        "\n",
        "#import data, plot data\n",
        "y_D, x_D = DataSet.get_data()\n",
        "DataSet.plot_data()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8ce51d3890ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset1_linreg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataSet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#import data, plot data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset1_linreg'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "JiCIrOeq1IiS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TGNUdVJJ16y-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JT5rfsfSZ1GC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Initialize a variable `w` which contains the model parameters. `w` should be a numpy array of dimensions (2,1), i.e., a column vector. `w[0][0]` $= w_0$ and `w[1][0]` $= w_1$.  Use $w_0 = w_1 = 0.1$. (you can use `type` method and `shape` property of numpy array to check if the variavble is of the expected type and shape)"
      ]
    },
    {
      "metadata": {
        "id": "3zf3KKxGbba6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# w = ...\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mCay_aM6b96B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2. Implement a function `predict_y1`, which takes an array of inputs $[x_1 \\dots x_n]$ and produces an array of correspoding outputs $[a(x_1) \\dots a(x_n)]$. Use the model $a(x) =w_1x+w_0$. The input is a numpy array of size $(M=1,n)$ and the output is a numpy array of size $(1,n)$. In the function you can use the global variable `w` containing the model parameters (from the previous point). In this case, the model will be automaticly updated when the value of `w` changes"
      ]
    },
    {
      "metadata": {
        "id": "Esrzcd4pdOgA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict_y1(x_arr):\n",
        "  # a = ... \n",
        "  \n",
        "  \n",
        "  return a "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wBccDp-Pd5G4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We plot the implemented model. If everything went right you should obtain the line $y=0.1x+0.1$ representing the model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3HcyjBvPeE0X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DataSet.plot_model(predict_y1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZKCZ1I91fimf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3. Implement a function `l2_cost` computing the quadratic cost for a given model. The inputs to the function are: `x_arr` = a numpy array of size $(1,n)$ containing inputs to the model; `y_arr` = a numpy array of size $(1,n)$ containing the correspoding outputs from the data set; `predict_f` = a function handle which represents the model (eg. a function `predict_y1`)"
      ]
    },
    {
      "metadata": {
        "id": "PC5-AGTYgKRH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def l2_cost(x_arr, y_arr, predict_f):\n",
        "    # cost = ...\n",
        "    \n",
        "    \n",
        "    return cost\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O1Y4buajh06n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We compute cost for the initialized model. (the cost should be around $3.36$ or of a similar magnitude if you used a different scaling than $0.5$ for the cost function)"
      ]
    },
    {
      "metadata": {
        "id": "-12gzZ4SiJo-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('cost: %.3f' % l2_cost(x_D, y_D, predict_y1))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H40UPwpkLAMp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "4. Compute the gradient for the current set of parameters. The gradient should be a vector of the same size as the vector `w` and should contain partial derivatives for the correspoding parameters. When correctly implemented you should obtain a vector $\\nabla  w \\approx [1.229, 2.231]^T$ (or something parallel if you used a different scaling than  0.5  for the cost function)"
      ]
    },
    {
      "metadata": {
        "id": "-kO9QsioLswh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# grad_w = ...\n",
        "\n",
        "print(grad_w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JovjKb4qNLhI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "5. Implement the gradient descent. You have to update `w` iterativley using the computed gradients. Note that `w` is a global variable used in the predicting function. Thus updating `w` automatically updates the predicting function. Use learning rate $\\approx 0.01$ and $\\approx 1000$ update steps.\n"
      ]
    },
    {
      "metadata": {
        "id": "dK9d5BTOOKT4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#for i in range(1000):\n",
        "  # ...\n",
        "  # w = w - 0.01 * grad_w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iw5UJmZCN9gv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We plot the model and compute the cost. Expected cost: $\\approx 0.87$"
      ]
    },
    {
      "metadata": {
        "id": "CQvs4hSmOF73",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DataSet.plot_model(predict_y1)\n",
        "print('Cost:%f' % l2_cost(x_D, y_D, predict_y1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FL6Rk9X-Oo6c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem 2 - Data Normalization\n",
        "\n",
        "In this exercise we extend the precious model by data normalization. Assume we have the data matrix (our matrix is $1 \\times N$ but here we consider the more general case $M \\times N$)\n",
        "\\begin{align}\n",
        "\\mat{X}_{\\text{data}} = \n",
        "\\begin{bmatrix}\n",
        "&x_{(1),1} \t&\\dots \t&x_{(1),N} \t\\\\\n",
        "&x_{(2),1} \t&\\dots \t&x_{(2),N} \t\\\\\n",
        "&\\vdots &\\dots  &\\vdots\\\\\n",
        "&x_{(M),1} \t&\\dots \t&x_{(M),N} \t\\\\\n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "The notation we use is as follows: $x_{(j),i}$ is a $j$-th entry in the $i$-th vector $\\vec{x}_i$from the data set. Each of the rows corresponds to a samples from different input variables $X_{(1)}, \\dots, X_{(M)}$. That is, we consider each entry in the vector  of input variables from the training set as being a different scalar variable. E.g., the second row of $\\mat{X}_{\\text{data}}$ contains the samples from the variable $X_{(2)}$, that is, the second entry of training data vector. Each of the variables $X_{(i)}$ may have different mean and variance. This may lead to overflow/underflow in computations as well as makes GD converge slower. A common approach in machine learning is to normalize the variables $X_{(1)}, \\dots, X_{(M)}$. This can be done by computing the mean $\\mu_i$ and the variance $\\sigma^2_i$ (or the standard deviation denoted by $\\sigma_i$ ) of each of the variables and transforming the variables in the following way\n",
        "\\begin{align}\n",
        "X_{(i)} \\gets \\frac{X_{(i)} - \\mu_i}{\\sigma_i}.\n",
        "\\end{align}\n",
        "This way each of the new variables has zero mean and unit variance. \n",
        "\n",
        "The normalization in our case can be performed by computing the mean and the variance of each row from $\\mat{X}_{\\text{data}}$ and  applying the normalization transformation to each of the samples in the row. After training the model with the normalized data, if we want to apply the model to new data, we need to perform the same normalization (with means and variances computed on the training data). That is, first we transform $\\vec{x}_\\text{new}$ just as we transformed all our training data. Next, we perform prediction on the transformed version of $\\vec{x}_\\text{new}$.\n",
        "\n",
        "Observe, that once we compute the mean and the variance for the training data, the data normalization is just a linear transformation of the input variables. Althoguh it does not change the model capabilities, it is often used in practice as it speeds up the training."
      ]
    },
    {
      "metadata": {
        "id": "c6kg-aFeSeot",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. Implement a fuction which computes and returns the mean value and the standard deviaton for each input variable (row of the data matrx) and returns them. The input is an numpy array of size $(M,n)$. The outputs are two numpy arrays of size $(M,1)$."
      ]
    },
    {
      "metadata": {
        "id": "SQn9Kj3bQ_Lm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_norm_params(x_data):\n",
        "  # mean_x      = ...\n",
        "  # stdd_x      = ... \n",
        "  \n",
        "  return mean_x, stdd_x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dxpz4XjaT1JG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We compute the normalization parameters for our dataset. We will store them as global variables and use them in the predictive function (just as the variable `w` is used). The mean should be around 0.414 and the stdd 0.248."
      ]
    },
    {
      "metadata": {
        "id": "t51Nv6VbUBzF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_D_mean, x_D_stdd = get_norm_params(x_D)\n",
        "print('mean:%.3f, stdd:%.3f' % (x_D_mean, x_D_stdd))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uv_c2PB6VH1m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2. Implement a function `predict_y1n`, which takes an array of inputs $[x_1 \\dots x_n]$ and produces an array of correspoding outputs $[a(x_1) \\dots a(x_n)]$. Use the model $a(x) =w_1x+w_0$. However,  now perform the data normalization before using the model. Normalize the input using the global variables coputed from the training data: `x_D_mean, x_D_stdd`.The input to the function is a numpy array of size $(M,n)$ and the output is a numpy array of size $(1,n)$. In the function you can use the global variable `w` containing the model parameters (from the previous point). In this case, the model will be automaticly updated when the value of `w` changes."
      ]
    },
    {
      "metadata": {
        "id": "D4xsezclW5LU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict_y1n(x):\n",
        "    # a = ...\n",
        "    \n",
        "    \n",
        "    return a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1k4-Bi3cXLr2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3. Compute the gradient for the initial parameters (you can re-run the cell from above which initializes  w, or just copy here the code from there)). Remeber to use the normalization. The gradient should be a vector of the same size as to the vector `w` and should contain partial derivatives for the correspoding parameters. When correctly implemented you should obtain a vector $\\nabla  w = [0.81, 7.69]^T$ (for $w=[0.1, 0.1]^T$."
      ]
    },
    {
      "metadata": {
        "id": "H7JPhDUKXSq1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#grad_w = ...\n",
        "\n",
        "\n",
        "print(grad_w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "A--B4eW8YbBh"
      },
      "cell_type": "markdown",
      "source": [
        "4. Implement the gradient descent. You have to update `w` iterativley using the computed gradients. Note that `w` is a global variable for the prediciton function and updating `w` autmatically updates the predictive model. Apply normalization to the data before using them for training. Use learning rate $\\approx 0.01$ and $\\approx 1000$ update steps.\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qGCWnzNcYqC5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#for i in range(1000):\n",
        "  # ...\n",
        "  # w = w - 0.01 * grad_w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rOHpxmYzY8QU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now plot the model and compute the cost. Expected cost: $\\approx 0.87$"
      ]
    },
    {
      "metadata": {
        "id": "PcgJxxK6ZCzM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DataSet.plot_model(predict_y1n)\n",
        "print('Cost:%f' % l2_cost(x_D, y_D, predict_y1n))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pwqnyrSIdu-7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem 3 - Linear Models for Non-linear Functions\n",
        "In this exercise we work with the previous dataset but we will extend our model to include non-linear functions of the input data. Observe, that so far our model was only able to reliably approxiatmate linear functions. The idea presented in this part of the tutorial is a powerful technieque which greatly improves  the variety of the functions which can be approximated. For this approach the data normalization is quite helpful. Otherwise GD may be inefficient.\n",
        "\n",
        "We will extend the model by introducing new input variables, which are functions of our base input variable. Recall that inputs in our data set are \n",
        "\\begin{align}\n",
        "\\mat{X}_{\\text{data}} = \n",
        "\\begin{bmatrix}\n",
        "&x_1 \t&\\dots \t&x_N \t\\\\\n",
        "\\end{bmatrix}.\n",
        "\\end{align}\n",
        "\n",
        "To make the model richer we introduce polynomial functions of the base data as new variables. We build artifically a new data set as follows:\n",
        "\\begin{align}\n",
        "\\mat{X}_{\\text{data}} = \n",
        "\\begin{bmatrix}\n",
        "&x_1 \t&\\dots \t&x_N \t\\\\\n",
        "&x_1^2&\\dots \t&x_N^2 \t\\\\\n",
        "&\\vdots &\\dots \t&\\vdots \t\\\\\n",
        "&x_1^P&\\dots \t&x_N^P \t\\\\\n",
        "\\end{bmatrix}.\n",
        "\\end{align}\n",
        "\n",
        "We treat each row of the newly created dataset as a new variable and perform regression accordingly. Consequetly, the parameters vector `w` shound now contain $P+1$ entries (one extra for the bias parameter $w_0$)."
      ]
    },
    {
      "metadata": {
        "id": "kMiAKKJy_rGY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. Implement the function `extend_data1D` which takes as an imput the numpy array of size $(1,n)$, e.g., the base array $X_\\text{data}$, and returns  the extended array of size  $(p,n)$ as explained above.  "
      ]
    },
    {
      "metadata": {
        "id": "BD_-JyElBLEg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def extend_data1D(x, p=1): \n",
        "    # x_ext = ...\n",
        "    \n",
        "    return x_ext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CKF_MB3JBTog",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We set $P=4$ (feel free to try different values as well). We compute the normalization parameters for the extended dataset. You should get `x_D_ext_mean` $= [0.41, 0.23, 0.14, 0.09]^T$ and x_D_ext_stdd $= [0.24, 0.20, 0.15, 0.11]^T$"
      ]
    },
    {
      "metadata": {
        "id": "FwH4ARU0B4Jc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "P = 4\n",
        "x_D_ext = extend_data1D(x_D, P)\n",
        "x_D_ext_mean, x_D_ext_stdd = get_norm_params(x_D_ext)\n",
        "\n",
        "print(x_D_ext_mean)\n",
        "print(x_D_ext_stdd)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rZvXtr4DCfY2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2. Implement a function `predict_y2`, which takes an array of inputs $[x_1 \\dots x_n]$ and produces an array of correspoding outputs $[a(x_1) \\dots a(x_n)]$. Use the model $a(x) = \\sum_{i=0}^P w_i x^i$ (you can use the extend function to generate terms $x^i, i>1$). Apply the data normalization before using the model. Normalize the inputs (which include the newly generated functions of the base input) using the global variables `x_D_ext_mean, x_D_ext_stdd`.The input is a numpy array of size $(M=1,n)$ and the output is a numpy array of size $(1,n)$. In the function you can use the global variable `w` containing the model parameters (from the previous point). In this case, the model will be automaticly updated when the value of `w` changes."
      ]
    },
    {
      "metadata": {
        "id": "E-BLL-qYD0Rf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict_y2(x):\n",
        "    # a = ...\n",
        "    \n",
        "    return a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V03r9hujE7Cu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We initialize the value of the parametes and plot the implemented model. It does not fit the data but it is clearly non-linear, which showes that the model is more representative."
      ]
    },
    {
      "metadata": {
        "id": "hqo4kAQRFIYV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w = np.array([0.1]*(P+1)).reshape(-1,1)\n",
        "\n",
        "DataSet.plot_model(predict_y2)\n",
        "print('Cost:%f' % l2_cost(x_D, y_D, predict_y2))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2332oiBoJ5I9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3. Compute the gradient for the current set of parameters. Remeber to use the normalization. The gradient should be a vector of the same size as to the vector `w` and should contain partial derivatives for the correspoding parameters. When correctly implemented you should obtain a vector $\\nabla  w = [0.81, 10.44, 11.15, 11.05, 10.63]^T$ (or anyting parallel if you used different scaling used for the cost function)"
      ]
    },
    {
      "metadata": {
        "id": "0MsUBUy4J6ZY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# grad_w = ... \n",
        "\n",
        "\n",
        "print(grad_w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PlVgJhFLQc0C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "4. Implement gradient descent. You have to update `w` iterativley using the computed gradients. Note that `w` is a global variable for the prediciton function and updating `w` autmatically updates the predictive model. Apply normalization to the data before using them for training. Use learning rate $\\approx 0.01$ and $\\approx 20000$ update steps."
      ]
    },
    {
      "metadata": {
        "id": "K48ZzKHlL1JM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#for i in range(20000):\n",
        "  # ...\n",
        "  # w = w - 0.01 * grad_w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sQ3Ct-ihQmsl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now plot the model and compute the cost. Expected cost: $\\approx 0.03$"
      ]
    },
    {
      "metadata": {
        "id": "nvj5AKq1OBWD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DataSet.plot_model(predict_y2)\n",
        "print('Cost:%f' % l2_cost(x_D, y_D, predict_y2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "saqRP2IzZ-BX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For linear models we can create new input variables by appling functions to the base input variables. Next, we treat the newly generated variables as regular ones and perform the regression. This technique allows to represent non-linear functions at the cost of more complicated model (more parameters to find, larger extended dataset). The problem is still linear because of the linear dependence on the parameters $w$. The optimization problem is still convex (if the cost function is convex) and  we are able to find the optimum `w` by using GD. One problem with the presented approach is that it requires expert-aided choice of the functions. For different regression problems, e.g., housing prices, speech processing, different functions may result in good peredicions. This choice of the function requires expertise in the domain of the problem."
      ]
    },
    {
      "metadata": {
        "id": "B-zvAiT9ma-v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Problem 4* - Non-linear Model Based on Sine Functions\n",
        "\n",
        "This problem contains different dataset for indepedent solving. We import the dataset dataset2_linreg.py and plot the data points. (Please upload the dataset file to the notebook)"
      ]
    },
    {
      "metadata": {
        "id": "LtdkDb-om1uC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset2_linreg import DataSet\n",
        "\n",
        "#import data, plot data\n",
        "y_D, x_D = DataSet.get_data()\n",
        "DataSet.plot_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e8J45zOMnRgO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Use the trick from the previous problem to extend the data as follows:\n",
        "\\begin{align}\n",
        "\t\\mat{X}_{\\text{data}} = \n",
        "\t\\begin{bmatrix}\n",
        "\t&x_1 \t\t\t\t\t&\\dots \t&x_N \t\t\\\\\n",
        "\t&\\sin 2\\pi\\frac{x_1}{x_{\\text{max}}} \t\t\t&\\dots \t&\\sin 2\\pi\\frac{x_N}{x_{\\text{max}}} \\\\\n",
        "\t&\\vdots \t\t\t&\t\t&\\vdots \\\\\n",
        "\t&\\sin 2\\pi(P-1)\\frac{x_1}{x_{\\text{max}}} \t&\\dots \t&\\sin 2\\pi(P-1)\\frac{x_N}{x_{\\text{max}}}\\\\\n",
        "\t&\\sin 2\\pi P \\frac{x_1}{x_{\\text{max}}} \t&\\dots \t&\\sin 2\\pi P \\frac{x_N}{x_{\\text{max}}}\\\\\n",
        "\t\\end{bmatrix},\n",
        "\t\\end{align}\n",
        "\twhere $x_{\\text{max}}$ is the sample from the training data with maximum value."
      ]
    },
    {
      "metadata": {
        "id": "1rqKi8Fgn5oG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. Implement the function which generates the extra input variables, implement the predictive model, and train the model. Find the appropriate learning rate and number of interations. Remeber to use the data normlaization. Plot the final model."
      ]
    },
    {
      "metadata": {
        "id": "qlf7xCnVrGvI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}